\chapter{Grundlagen statistischer Methoden}

Bei der Regressionsanalyse geht es im Allgemeinen darum, das Verhalten einer Größe $Y$ in Abhängigkeit einer oder mehrerer anderer Größen $X_1, X_2, \dots, X_n$ zu modellieren. Die Größe $Y$ wird abhängig genannt, die Größen $X_i$ nennt man unabhängig. Für diese Arbeit wollen wir zunächst einige Annahnmen über diese voraussetzen. Diese Punkte gelten immer, falls nicht explizit etwas anderes festgelegt wird.
\begin{itemize}
    \item Die genannten Größen sind Zufallsvariablen. Das sind Funktionen deren Werte die Ergebnisse eines Zufallsvorgangs darstellen.
    \item Die Zufallsvariablen sind auf der Menge $M = \{1, \dots, m\}$ definiert und bilden in die reellen Zahlen ab:
    \begin{align*}
        Y: M \rightarrow \mathbb{R},~~ X_1: M \rightarrow \mathbb{R},~~ \dots,~~ X_n: M \rightarrow \mathbb{R}
    \end{align*}
    Das bedeutet die Zufallsvariablen sind metrisch skaliert. Die $m$ Zahlen in der Menge $M$ entsprechen den $m$ Datenpunkten, die wir als Datenbasis für die Regressionsanalyse besitzen.
    \item Wir verwenden die folgenden Abkürzungen für die Werte der Zufallsvariablen:
    \begin{align*}
        y_i &:= Y(i) ~~~\text{für alle}~ i \in M,\\
        x_{i, j} &:= X_j(i) ~~~\text{für alle}~ i \in M ~~\text{und}~ 1 \leq j \leq n
    \end{align*}
    \item Einen Datenpunkt aus unserer Datenbasis fassen wir als Vektor der Länge $(n + 1)$ auf. Damit lässt sich die Datenbasis schreiben als:
    \begin{align*}
        (y_1, x_{1,1}, \dots, x_{1,n}), \dots, (y_m, x_{m,1}, \dots, x_{m,n})
    \end{align*}
\end{itemize}
Das Modell definieren wir anhand einer Funktion $f$, welche für Werte der unabhängigen Variablen einen geschätzten Wert für die abhängige Variable liefert. Idealerweise existiert eine Funktion, die zum Einen eine einfache Darstellung (z.B. durch eine arithmetische Formel) besitzt und zum Anderen alle unabhängigen Werte der Datenmenge exakt prognostiziert. Das bedeutet:
\begin{align*}
    y_i = f(x_{i, 1}, \dots, x_{i, n}) ~~~\text{für alle}~~ 1 \leq i \leq m
\end{align*}
Falls eine Formel wie hier für alle Datenpunkte gelten soll, verwenden wir als Abkürzung auch die Zufallsvariablen selbst, also:
\begin{align*}
    Y = f(X_1, \dots, X_N)
\end{align*}
Im Allgemeinen ist es nicht möglich eine Funktion $f$ zu finden, die beide Eigenschaften erfüllt. Man versucht also eine Funktion mit einer möglichst einfachen Form zu finden, die die Datenmenge möglichst gut approximiert. Wir definieren für jeden Datenpunkt den Fehler $e_i$, der sich durch die nicht exakte Modellfunktion $f$ ergibt:
\begin{align*}
    e_i = y_i - f(x_{i, 1}, \dots, x_{i, n})
\end{align*}
Ziel der Regressionsanalyse ist es nun eine Funktion $f$ zu finden, die diese Fehlerterme minimiert. Diese Optimierung geschieht global, also für die gesamte Datenmenge und nicht nur für einzelne Datenpunkte.

\section{Lineare Regression}

Bei der linearen Regression geht man von einem linearen Zusammenhang zwischen der abhängigen und den unabhängigen Variablen aus. Die Funktion $f$ ist also von folgender Form:
\begin{align*}
    f(x_1, \dots, x_n) = \alpha + \sum_{i=1}^n \beta_i \cdot x_i ~~~ \text{mit} ~~ \beta_i \in \mathbb{R}
\end{align*}
Das Maß für die Qualität einer Funktion $f$ definiert durch die Parameter $\alpha, \beta_1, \dots, \beta_n$ ist die Summe der quadrierten Fehlerterme:
\begin{align*}
    E(\alpha, \beta_1, \dots, \beta_n) = \sum_{j=1}^m e_j^2 =  \sum_{j=1}^m \left( y_j - f(x_{j, 1}, \dots, x_{j, n}) \right)^2 = \sum_{j=1}^m \left( y_j - \alpha - \sum_{i=1}^n \beta_i \cdot x_{i, j} \right)^2
\end{align*}
Wir suchen also die Parameter $\hat\alpha, \hat\beta_1, \dots, \hat\beta_n$ für die gilt:
\begin{align*}
    E(\hat\alpha, \hat\beta_1, \dots, \hat\beta_n) = min \left\{E(\alpha, \beta_1, \dots, \beta_n) ~|~ \alpha \in \mathbb{R}, \beta_1 \in \mathbb{R}, \dots, \beta_n \in \mathbb{R} \right \}
\end{align*}
Um dieses Minimierungsproblem zu lösen berechnen wir die partiellen Ableitungen von $E$.
\begin{align*}
    \dfrac{\partial E}{\partial \alpha} &= - 2 \cdot \sum_{j=1}^m \left( y_j - f(x_{j, 1}, \dots, x_{j, n}) \right) = - 2 \cdot \sum_{j=1}^m \left( y_j - \alpha - \sum_{i=1}^n \beta_i \cdot x_{i, j} \right)\\
    \dfrac{\partial E}{\partial \beta_k} &= - 2 \cdot \sum_{j=1}^m x_{k, j} \cdot \left( y_j - f(x_{j, 1}, \dots, x_{j, n}) \right)\\
    &= - 2 \cdot \sum_{j=1}^m x_{k, j} \cdot \left( y_j - \alpha - \sum_{i=1}^n \beta_i \cdot x_{i, j} \right) ~~~\text{für}~~ 1 \leq k \leq n
\end{align*}
Durch Nullsetzen der partiellen Ableitungen erhält man ein lineares Gleichungssystem mit $(n + 1)$ Gleichungen und ebensovielen Unbekannten.
\begin{align*}
    \dfrac{\partial E}{\partial \alpha} = 0, ~~ \dfrac{\partial E}{\partial \beta_1} = 0, ~~ \dots, ~~ \dfrac{\partial E}{\partial \beta_n} = 0
\end{align*}
Die Lösung dieses Gleichungssystems (falls eine existent) ist das gesuchte Minimum.

\subsection{Einfache lineare Regression}

Man spricht von einfacher linearer Regression, wenn man mit nur eine unabhängige Variable arbeitet. Anschaulich möchte mann hier die bestmögliche Schätzgerade durch eine gegebene Punktwolke legen.

Wir nennen die unabhängige Variable in diesem Kapitel statt $X_1$ einfach nur $X$. Ebenso schreiben wir $\beta_1 = \beta$ und $x_{1, j} = x_j$. Dann können wir das lineare Gleichungssystem zum Auffinden des Minimums explizit aufschreiben:
\begin{align*}
    0 &= - 2 \cdot \sum_{j=1}^m (y_i - \alpha - \beta \cdot x_{j})\\
    0 &= - 2 \cdot \sum_{j=1}^m x_j \cdot (y_i - \alpha - \beta \cdot x_{j})
\end{align*}
Für dieses Gleichungssystem kann die Lösung explizit angegeben werden, wobei wir hier nicht näher auf die Herleitung dieses Ergebnisses eingehen wollen:
\begin{align*}
    \hat\beta &= \dfrac{\sum\limits_{j=1}^m (x_j - \bar{x})(y_j - \bar{y})}{\sum\limits_{j=1}^m (x_j - \bar{x})^2}\\
    \hat\alpha &= \bar{y} - \hat\beta \bar{x}
\end{align*}
Dabei bezeichnen $\bar{x}$ und $\bar{y}$ die Mittelwerte von $X$ respektive $Y$.

\subsection{Multible lineare Regression}

Bei multibler linearer Regression existieren mindestens zwei unabhängige Variablen. Hier ist es nicht mehr zweckmäßig eine explizite Lösung anzugeben. Hier sind alternative Methoden zur Berechnung der Parameter nötig.

Neben einer Vielzahl von Algorithmen, die ein Optimierungsproblem iterativ lösen, gibt es auch die Möglichkeit der Berechnung durch Matrizenmultiplikation. Definieren wir dazu die folgenden Matrizen und Vektoren:
\begin{align*}
    X &= \begin{pmatrix}
        1 & x_{1, 1} & \dots & x_{1, n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m, 1} & \dots & x_{m, n}
    \end{pmatrix} \in \mathbb{R}^{m \times (n + 1)} \\
    y &= \begin{pmatrix}
        y_1 \\
        \vdots \\
        y_m
    \end{pmatrix} \in \mathbb{R}^{m \times 1}, ~~~
    b = \begin{pmatrix}
        \hat\alpha \\
        \hat\beta_1 \\
        \vdots \\
        \hat\beta_n
    \end{pmatrix} \in \mathbb{R}^{(n + 1) \times 1}
\end{align*}
Dabei ist $b$ der Vektor mit gesuchten Parametern für die Minimierung der kleinsten Quadrate. Falls die Matrix $X^T X$ invertiertbar ist, gilt die folgende Formel für die Berechnung der gesuchten Parameter:
\begin{align*}
    b = (X^T X)^{-1} X^T y
\end{align*}
