\chapter{Grundlagen statistischer Methoden}

Bei der Regressionsanalyse geht es im Allgemeinen darum, das Verhalten einer Größe $Y$ in Abhängigkeit einer oder mehrerer anderer Größen $X_1, X_2, \dots, X_n$ zu modellieren. Die Größe $Y$ wird abhängig genannt, die Größen $X_i$ nennt man unabhängig. Für diese Arbeit wollen wir zunächst einige Annahnmen über diese voraussetzen. Diese Punkte gelten immer, falls nicht explizit etwas anderes festgelegt wird.
\begin{itemize}
    \item Die genannten Größen sind Zufallsvariablen. Das sind Funktionen deren Werte die Ergebnisse eines Zufallsvorgangs darstellen.
    \item Die Zufallsvariablen sind auf der Menge $M = \{1, \dots, m\}$ definiert und bilden in die reellen Zahlen ab:
    \begin{align*}
        Y: M \rightarrow \mathbb{R},~~ X_1: M \rightarrow \mathbb{R},~~ \dots,~~ X_n: M \rightarrow \mathbb{R}
    \end{align*}
    Das bedeutet die Zufallsvariablen sind metrisch skaliert. Die $m$ Zahlen in der Menge $M$ entsprechen den $m$ Datenpunkten, die wir als Datenbasis für die Regressionsanalyse besitzen.
    \item Wir verwenden die folgenden Abkürzungen für die Werte der Zufallsvariablen:
    \begin{align*}
        y_i &:= Y(i) ~~~\text{für alle}~ i \in M,\\
        x_{i, j} &:= X_j(i) ~~~\text{für alle}~ i \in M ~~\text{und}~ 1 \leq j \leq n
    \end{align*}
    \item Einen Datenpunkt aus unserer Datenbasis fassen wir als Vektor der Länge $(n + 1)$ auf. Damit lässt sich die Datenbasis schreiben als:
    \begin{align*}
        (y_1, x_{1,1}, \dots, x_{1,n}), \dots, (y_m, x_{m,1}, \dots, x_{m,n})
    \end{align*}
\end{itemize}
Das Modell definieren wir anhand einer Funktion $f$, welche für Werte der unabhängigen Variablen einen geschätzten Wert für die abhängige Variable liefert. Idealerweise existiert eine Funktion, die zum Einen eine einfache Darstellung (z.B. durch eine arithmetische Formel) besitzt und zum Anderen alle unabhängigen Werte der Datenmenge exakt prognostiziert. Das bedeutet:
\begin{align*}
    y_i = f(x_{i, 1}, \dots, x_{i, n}) ~~~\text{für alle}~~ 1 \leq i \leq m
\end{align*}
Falls eine Formel wie hier für alle Datenpunkte gelten soll, verwenden wir als Abkürzung auch die Zufallsvariablen selbst, also:
\begin{align*}
    Y = f(X_1, \dots, X_N)
\end{align*}
Im Allgemeinen ist es nicht möglich eine Funktion $f$ zu finden, die beide Eigenschaften erfüllt. Man versucht also eine Funktion mit einer möglichst einfachen Form zu finden, die die Datenmenge möglichst gut approximiert. Wir definieren für jeden Datenpunkt den Fehler $e_i$, der sich durch die nicht exakte Modellfunktion $f$ ergibt:
\begin{align*}
    e_i = y_i - f(x_{i, 1}, \dots, x_{i, n})
\end{align*}
Ziel der Regressionsanalyse ist es nun eine Funktion $f$ zu finden, die diese Fehlerterme minimiert. Diese Optimierung geschieht global, also für die gesamte Datenmenge und nicht nur für einzelne Datenpunkte.

\section{Lineare Regression}

Bei der linearen Regression geht man von einem linearen Zusammenhang zwischen der abhängigen und den unabhängigen Variablen aus. Die Funktion $f$ ist also von folgender Form:
\begin{align*}
    f(x_1, \dots, x_n) = \alpha + \sum_{i=1}^n \beta_i \cdot x_i ~~~ \text{mit} ~~ \beta_i \in \mathbb{R}
\end{align*}
Das Maß für die Qualität einer Funktion $f$ definiert durch die Parameter $\alpha, \beta_1, \dots, \beta_n$ ist die Summe der quadrierten Fehlerterme:
\begin{align*}
    E(\alpha, \beta_1, \dots, \beta_n) = \sum_{j=1}^m e_j^2 =  \sum_{j=1}^m \left( y_j - f(x_{j, 1}, \dots, x_{j, n}) \right)^2 = \sum_{j=1}^m \left( y_j - \alpha - \sum_{i=1}^n \beta_i \cdot x_{i, j} \right)^2
\end{align*}
Wir suchen also die Parameter $\hat\alpha, \hat\beta_1, \dots, \hat\beta_n$ für die gilt:
\begin{align*}
    E(\hat\alpha, \hat\beta_1, \dots, \hat\beta_n) = min \left\{E(\alpha, \beta_1, \dots, \beta_n) ~|~ \alpha \in \mathbb{R}, \beta_1 \in \mathbb{R}, \dots, \beta_n \in \mathbb{R} \right \}
\end{align*}
Um dieses Minimierungsproblem zu lösen berechnen wir die partiellen Ableitungen von $E$.
\begin{align*}
    \dfrac{\partial E}{\partial \alpha} &= - 2 \cdot \sum_{j=1}^m \left( y_j - f(x_{j, 1}, \dots, x_{j, n}) \right) = - 2 \cdot \sum_{j=1}^m \left( y_j - \alpha - \sum_{i=1}^n \beta_i \cdot x_{i, j} \right)\\
    \dfrac{\partial E}{\partial \beta_k} &= - 2 \cdot \sum_{j=1}^m x_{k, j} \cdot \left( y_j - f(x_{j, 1}, \dots, x_{j, n}) \right)\\
    &= - 2 \cdot \sum_{j=1}^m x_{k, j} \cdot \left( y_j - \alpha - \sum_{i=1}^n \beta_i \cdot x_{i, j} \right) ~~~\text{für}~~ 1 \leq k \leq n
\end{align*}
Durch Nullsetzen der partiellen Ableitungen erhält man ein lineares Gleichungssystem mit $(n + 1)$ Gleichungen und ebensovielen Unbekannten.
\begin{align*}
    \dfrac{\partial E}{\partial \alpha} = 0, ~~ \dfrac{\partial E}{\partial \beta_1} = 0, ~~ \dots, ~~ \dfrac{\partial E}{\partial \beta_n} = 0
\end{align*}
Die Lösung dieses Gleichungssystems (falls eine existent) ist das gesuchte Minimum.

\subsection{Einfache lineare Regression}

Man spricht von einfacher linearer Regression, wenn man mit nur eine unabhängige Variable arbeitet. Anschaulich möchte mann hier die bestmögliche Schätzgerade durch eine gegebene Punktwolke legen.

Wir nennen die unabhängige Variable in diesem Kapitel statt $X_1$ einfach nur $X$. Ebenso schreiben wir $\beta_1 = \beta$ und $x_{1, j} = x_j$. Dann können wir das lineare Gleichungssystem zum Auffinden des Minimums explizit aufschreiben:
\begin{align*}
    0 &= - 2 \cdot \sum_{j=1}^m (y_i - \alpha - \beta \cdot x_{j})\\
    0 &= - 2 \cdot \sum_{j=1}^m x_j \cdot (y_i - \alpha - \beta \cdot x_{j})
\end{align*}
Für dieses Gleichungssystem kann die Lösung explizit angegeben werden, wobei wir hier nicht näher auf die Herleitung dieses Ergebnisses eingehen wollen:
\begin{align*}
    \hat\beta &= \dfrac{\sum\limits_{j=1}^m (x_j - \bar{x})(y_j - \bar{y})}{\sum\limits_{j=1}^m (x_j - \bar{x})^2}\\
    \hat\alpha &= \bar{y} - \hat\beta \bar{x}
\end{align*}
Dabei bezeichnen $\bar{x}$ und $\bar{y}$ die Mittelwerte von $X$ respektive $Y$.

\subsection{Multible lineare Regression}

Bei multibler linearer Regression existieren mindestens zwei unabhängige Variablen. Hier ist es nicht mehr zweckmäßig eine explizite Lösung anzugeben. Hier sind alternative Methoden zur Berechnung der Parameter nötig.

Neben einer Vielzahl von Algorithmen, die ein Optimierungsproblem iterativ lösen, gibt es auch die Möglichkeit die Parameter durch Matrizenmultiplikation zu berechnen. Definieren wir dazu die folgenden Matrizen und Vektoren:
\begin{align*}
    X &= \begin{pmatrix}
        1 & x_{1, 1} & \dots & x_{1, n} \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & x_{m, 1} & \dots & x_{m, n}
    \end{pmatrix} \in \mathbb{R}^{m \times (n + 1)} \\
    y &= \begin{pmatrix}
        y_1 \\
        \vdots \\
        y_m
    \end{pmatrix} \in \mathbb{R}^{m \times 1}, ~~~
    b = \begin{pmatrix}
        \hat\alpha \\
        \hat\beta_1 \\
        \vdots \\
        \hat\beta_n
    \end{pmatrix} \in \mathbb{R}^{(n + 1) \times 1}
\end{align*}
Dabei ist $b$ der Vektor mit gesuchten Parametern für die Minimierung der kleinsten Quadrate. Falls die Matrix $X^T X$ invertiertbar ist, gilt die folgende Formel für die Berechnung der gesuchten Parameter:
\begin{align*}
    b = (X^T X)^{-1} X^T y
\end{align*}

\section{Logistische Regression}

Die logistische Regression findet Anwendung im Falle, dass die abhängige Variable eine binäre Variable ist, also eine Variable, die nur zwei Werte annehmen kann. Oft handelt es sich um eine Eigenschaft, die ein bestimmter Datensatz besitzt oder nicht, wie zum Beispiel ein Premium-Abonnement für eine Web-Service oder der Besitz eines Auto. Auch das Geschlecht einer Person ist ein Beispiel für eine binäre Variable. Wir bezeichnen die beiden möglichen Werte einer solchen Variablen hier immer mit $0$ und $1$. Die Zuordnung vom Merkmal zur Zahl ist frei wählbar.

Lineare Regression eignet sich oft nicht zur Modellierung einer binären Variablen, da eine lineare Funktion in der Regel unbeschränkt ist, also insbesondere Werte größer als 1 und kleiner als 0 annimmt. Um diesem Problem abzuhelfen wählen wir eine Funktion, die beliebige Zahlen auf das Interval $[0, 1]$ abbildet. Im Falle der logistischen Regression verwendet man die gleichnamige logistische Funktion:
\begin{align*}
    l: \mathbb{R} \rightarrow (0, 1),~~ x \mapsto \dfrac{1}{1+e^{-x}}
\end{align*}
Diese Funktion wendet man nun auf die Linearkombination aller unabhängigen Variablen mit Parametern $\beta_1, \dots, \beta_n$ und konstantem Term $\alpha$ an. Das Ergebnis für den $i$-ten Datensatz bezeichnen wir mit $\pi_i$
\begin{align*}
    \pi_i = \pi_i(\alpha, \beta_1, \dots, \beta_n) := l \left( \alpha + \sum_{j=1}^n \beta_j \cdot x_{i, j} \right) = \left( 1 + \exp \left( - \alpha - \sum_{j=1}^n \beta_j \cdot x_{i, j} \right) \right)^{-1}
\end{align*}
Anschaulich repräsentiert $\pi_i$ die Wahrscheinlichkeit dafür, dass die abhängige Variable eines Datensatzes mit unabhängigen Variablen $x_{i, 1}, \dots, x_{i, n}$ gleich $1$ ist, also:
\begin{align*}
    \pi_i = P(Y_i = 1 | X_1 = x_{i, 1}, \dots, X_n = x_{i, n})
\end{align*}
Man möchte die Parameter $\alpha, \beta_1, \dots, \beta_n$ nun so schätzen, dass die Wahrscheinlichkeit für das Auftreten der vorhandenen Datenbasis maximiert wird. Diese Wahrscheinlichkeit ist gegeben durch:
\begin{align*}
    L(\alpha, \beta_1, \dots, \beta_n) = \prod_{i=1}^m P(Y_i = y_i | X_1 = x_{i, 1}, \dots, X_n = x_{i, n}) = \prod_{i=1}^m y_i \cdot \pi_i(\alpha, \beta_1, \dots, \beta_n)
\end{align*}
Dieses Verfahren bezeichnet man auch als Maximum-Likelihood-Methode. Die Funktion $L$ nennt man dementsprechend auch Likelihoodfunktion. Oft maximiert man nicht $L$ direkt, sondern eher $\ln(L)$. Der Sinn ist, dass man das Produkt damit in eine Summe einzelner Logarithmen umwandeln kann, welche wiederum einfacher abzuleiten ist. Das darf man machen, da der Logarithmus eine stetig wachsende Funktion ist und die Werte von $L$ stets zwischen $0$ und $1$ liegen.

Wir suchen also die Parameter $\hat\alpha, \hat\beta_1, \dots, \hat\beta_n$ mit:
\begin{align*}
    L(\hat\alpha, \hat\beta_1, \dots, \hat\beta_n) = \max \left\{ L(\alpha, \beta_1, \dots, \beta_n) ~|~ \alpha \in \mathbb{R}, \beta_1 \in \mathbb{R}, \dots, \beta_n \in \mathbb{R} \right\}
\end{align*}
In diesem Fall kommt man leider nicht mehr an einer iterativen Lösung vorbei, da die partiellen Ableitungen und das entstehende lineare Gleichungssystem nicht mehr exakt lösbar sind. Eine der einfachsten Methoden zur Lösung von Optimierungsproblemen ist das Gradientenverfahren, welches im kommenden Teilkapitel kurz eingeführt wird.
